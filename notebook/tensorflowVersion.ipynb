{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb39c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:19:43.598437: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-16 18:19:43.617114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760631583.638500  274526 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760631583.645635  274526 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760631583.662284  274526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760631583.662304  274526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760631583.662308  274526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760631583.662310  274526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-16 18:19:43.667463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/lscblack/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lscblack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth enabled\n",
      "TensorFlow version: 2.19.0\n",
      "Num GPUs Available: 1\n",
      "🚀 Health Q&A Chatbot - TensorFlow Optimized Version\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Loading data from ../dataset/merged_health_dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preprocessing dataframe...\n",
      "INFO:__main__:Dropped 0 rows with missing values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (16371, 4)\n",
      "Columns: ['Question', 'Answer', 'topic', 'split']\n",
      "\n",
      "Topic distribution:\n",
      "topic\n",
      "growth_hormone_receptor          5430\n",
      "Genetic_and_Rare_Diseases        5388\n",
      "Diabetes_Digestive_Kidney        1157\n",
      "Neurological_Disorders_Stroke    1088\n",
      "Other                             981\n",
      "SeniorHealth                      769\n",
      "cancer                            729\n",
      "Heart_Lung_Blood                  559\n",
      "Disease_Control_Prevention        270\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:After length filtering: 16370 rows\n",
      "INFO:__main__:After deduplication: 16357 rows\n",
      "INFO:__main__:Training samples: 4000\n",
      "INFO:__main__:Validation samples: 666\n",
      "INFO:__main__:Test samples: 334\n",
      "INFO:__main__:Building retrieval system...\n",
      "INFO:__main__:Building FAISS index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample processed data:\n",
      "                                      Question_clean  \\\n",
      "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
      "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
      "2  What are the symptoms of Non-Small Cell Lung C...   \n",
      "\n",
      "                                        Answer_clean  \n",
      "0  - Non-small cell lung cancer is a disease in w...  \n",
      "1  Smoking is the major risk factor for non-small...  \n",
      "2  Signs of non-small cell lung cancer include a ...  \n",
      "\n",
      "📈 Preparing data splits...\n",
      "Using subset of 5000 samples for faster training\n",
      "\n",
      "🔍 Building retrieval system...\n",
      "Building index with 15807 unique answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 988/988 [00:20<00:00, 47.06it/s] \n",
      "INFO:__main__:FAISS index built with 15807 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing retrieval system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 79.43it/s]\n",
      "INFO:__main__:Training generative model...\n",
      "INFO:__main__:Building tokenizers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are symptoms of depression?'\n",
      "Retrieved 5 results:\n",
      "  1. Score: 0.698\n",
      "     Answer: Common Symptoms There are many symptoms associated with depression, and some will vary depending on ...\n",
      "  2. Score: 0.697\n",
      "     Answer: Symptoms of depression often vary depending upon the person. Common symptoms include - feeling nervo...\n",
      "\n",
      "🧠 Training generative model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Question vocab size: 2940\n",
      "INFO:__main__:Answer vocab size: 17317\n",
      "INFO:__main__:Starting model training...\n",
      "INFO:__main__:Building model architecture...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (4000, 128), (4000, 128)\n",
      "Validation data shape: (666, 128), (666, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760631621.577880  274526 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5909 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "INFO:__main__:Model built successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"health_qa_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"health_qa_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">376,448</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,216,704</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_ls… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional_ls… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_ls… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional_ls… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ decoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ bidirectional_ls… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,125,605</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">17317</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m376,448\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_lstm  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ encoder_embeddin… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,216,704\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_ls… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional_ls… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_ls… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional_ls… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m,      │    \u001b[38;5;34m394,240\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ bidirectional_ls… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_layer[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m32,832\u001b[0m │ concat_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ outputs (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m,       │  \u001b[38;5;34m1,125,605\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│                     │ \u001b[38;5;34m17317\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,408,997</span> (16.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,408,997\u001b[0m (16.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,408,997</span> (16.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,408,997\u001b[0m (16.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error in main execution: Exception encountered when calling Attention.call().\n",
      "\n",
      "\u001b[1mDimensions must be equal, but are 128 and 8 for '{{node health_qa_model_1/attention_layer_1/sub}} = Sub[T=DT_FLOAT](health_qa_model_1/attention_layer_1/MatMul, health_qa_model_1/attention_layer_1/mul)' with input shapes: [8,128,128], [8,128].\u001b[0m\n",
      "\n",
      "Arguments received by Attention.call():\n",
      "  • inputs=['tf.Tensor(shape=(8, 128, 256), dtype=float32)', 'tf.Tensor(shape=(8, 128, 256), dtype=float32)']\n",
      "  • mask=['tf.Tensor(shape=(8, 128), dtype=bool)', 'tf.Tensor(shape=(8, 128), dtype=bool)']\n",
      "  • training=True\n",
      "  • return_attention_scores=False\n",
      "  • use_causal_mask=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: Exception encountered when calling Attention.call().\n",
      "\n",
      "\u001b[1mDimensions must be equal, but are 128 and 8 for '{{node health_qa_model_1/attention_layer_1/sub}} = Sub[T=DT_FLOAT](health_qa_model_1/attention_layer_1/MatMul, health_qa_model_1/attention_layer_1/mul)' with input shapes: [8,128,128], [8,128].\u001b[0m\n",
      "\n",
      "Arguments received by Attention.call():\n",
      "  • inputs=['tf.Tensor(shape=(8, 128, 256), dtype=float32)', 'tf.Tensor(shape=(8, 128, 256), dtype=float32)']\n",
      "  • mask=['tf.Tensor(shape=(8, 128), dtype=bool)', 'tf.Tensor(shape=(8, 128), dtype=bool)']\n",
      "  • training=True\n",
      "  • return_attention_scores=False\n",
      "  • use_causal_mask=False\n",
      "Trying to continue with retrieval-only mode...\n",
      "\n",
      "🔄 Falling back to retrieval-only mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 231.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are common health tips?\n",
      "A: Treatment and prevention for P.A.D. often includes making long-lasting lifestyle changes, such as - quitting smoking - lowering blood pressure - lowering high blood cholesterol levels - lowering high blood glucose levels if you have diabetes - getting regular physical activity - following a healthy eating plan that's low in total fat, saturated fat, trans fat, cholesterol, and sodium (salt). quitting smoking lowering blood pressure lowering high blood cholesterol levels lowering high blood glucose levels if you have diabetes getting regular physical activity following a healthy eating plan that's low in total fat, saturated fat, trans fat, cholesterol, and sodium (salt). Two examples of healthy eating plans are Therapeutic Lifestyle Changes (TLC) and Dietary Approaches to Stop Hypertension (DASH).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Health Q&A Chatbot - TensorFlow Optimized Notebook (SIMPLIFIED VERSION)\n",
    "# Optimized for 6GB GPU, 32GB RAM, 24 cores\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q tensorflow sentence-transformers faiss-cpu pandas nltk scikit-learn matplotlib\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, Concatenate, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import html\n",
    "\n",
    "# Vector search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set memory growth to avoid GPU memory issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Configuration optimized for your hardware - SIMPLIFIED\n",
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'max_sequence_length': 128,\n",
    "    'embedding_dim': 128,\n",
    "    'lstm_units': 128,\n",
    "    'dense_units': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'max_vocab_size': 20000,\n",
    "    'max_answers': 3,\n",
    "    'similarity_threshold': 0.3,\n",
    "    'epochs': 15  # Reduced for quick training\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HealthDataProcessor:\n",
    "    \"\"\"Optimized data processor for health Q&A dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, max_sequence_length=128, max_vocab_size=20000):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.question_tokenizer = None\n",
    "        self.answer_tokenizer = None\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # HTML unescape and basic cleaning\n",
    "        text = html.unescape(text)\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "        text = re.sub(r'Key Points[:\\\\s-]*', '', text, flags=re.I)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess the entire dataframe\"\"\"\n",
    "        logger.info(\"Preprocessing dataframe...\")\n",
    "        \n",
    "        # Create copies to avoid SettingWithCopyWarning\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Drop missing values\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['Question', 'Answer']).reset_index(drop=True)\n",
    "        logger.info(f\"Dropped {initial_count - len(df)} rows with missing values\")\n",
    "        \n",
    "        # Clean text\n",
    "        df['Question_clean'] = df['Question'].apply(self.clean_text)\n",
    "        df['Answer_clean'] = df['Answer'].apply(self.clean_text)\n",
    "        \n",
    "        # Filter very short Q/A\n",
    "        df = df[(df['Question_clean'].str.len() > 10) & \n",
    "                (df['Answer_clean'].str.len() > 20)].reset_index(drop=True)\n",
    "        logger.info(f\"After length filtering: {len(df)} rows\")\n",
    "        \n",
    "        # Normalize topics\n",
    "        if 'topic' in df.columns:\n",
    "            df['topic'] = df['topic'].astype(str).str.lower().str.strip()\n",
    "            df['topic'] = df['topic'].str.replace(r'[^a-z0-9_ ]', '', regex=True)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['Question_clean', 'Answer_clean']).reset_index(drop=True)\n",
    "        logger.info(f\"After deduplication: {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def build_tokenizers(self, questions: List[str], answers: List[str]):\n",
    "        \"\"\"Build tokenizers for questions and answers\"\"\"\n",
    "        logger.info(\"Building tokenizers...\")\n",
    "        \n",
    "        self.question_tokenizer = Tokenizer(\n",
    "            num_words=self.max_vocab_size, \n",
    "            oov_token='<OOV>',\n",
    "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "        )\n",
    "        self.answer_tokenizer = Tokenizer(\n",
    "            num_words=self.max_vocab_size,\n",
    "            oov_token='<OOV>',\n",
    "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "        )\n",
    "        \n",
    "        self.question_tokenizer.fit_on_texts(questions)\n",
    "        self.answer_tokenizer.fit_on_texts(answers)\n",
    "        \n",
    "        logger.info(f\"Question vocab size: {len(self.question_tokenizer.word_index)}\")\n",
    "        logger.info(f\"Answer vocab size: {len(self.answer_tokenizer.word_index)}\")\n",
    "    \n",
    "    def texts_to_sequences(self, questions: List[str], answers: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Convert texts to padded sequences\"\"\"\n",
    "        question_sequences = self.question_tokenizer.texts_to_sequences(questions)\n",
    "        answer_sequences = self.answer_tokenizer.texts_to_sequences(answers)\n",
    "        \n",
    "        X = pad_sequences(question_sequences, maxlen=self.max_sequence_length, padding='post')\n",
    "        y = pad_sequences(answer_sequences, maxlen=self.max_sequence_length, padding='post')\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "class SimpleHealthModel:\n",
    "    \"\"\"Simplified Health Q&A Model using TensorFlow - NO ATTENTION\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.data_processor = None\n",
    "        \n",
    "    def build_model(self, question_vocab_size: int, answer_vocab_size: int) -> Model:\n",
    "        \"\"\"Build a SIMPLIFIED seq2seq model for health Q&A - NO ATTENTION\"\"\"\n",
    "        logger.info(\"Building SIMPLIFIED model architecture...\")\n",
    "        \n",
    "        # ===== SIMPLIFIED ENCODER =====\n",
    "        encoder_inputs = Input(shape=(self.config['max_sequence_length'],), name='encoder_inputs')\n",
    "        encoder_embedding = Embedding(\n",
    "            input_dim=question_vocab_size + 1,\n",
    "            output_dim=self.config['embedding_dim'],\n",
    "            mask_zero=True,\n",
    "            name='encoder_embedding'\n",
    "        )(encoder_inputs)\n",
    "        \n",
    "        # Single LSTM layer for encoder (simpler)\n",
    "        encoder_lstm = LSTM(\n",
    "            self.config['lstm_units'], \n",
    "            return_sequences=False,  # Only return final state\n",
    "            return_state=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.1,\n",
    "            name='encoder_lstm'\n",
    "        )\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        # ===== SIMPLIFIED DECODER =====\n",
    "        decoder_inputs = Input(shape=(self.config['max_sequence_length'],), name='decoder_inputs')\n",
    "        decoder_embedding = Embedding(\n",
    "            input_dim=answer_vocab_size + 1,\n",
    "            output_dim=self.config['embedding_dim'],\n",
    "            mask_zero=True,\n",
    "            name='decoder_embedding'\n",
    "        )(decoder_inputs)\n",
    "        \n",
    "        # Single LSTM layer for decoder\n",
    "        decoder_lstm = LSTM(\n",
    "            self.config['lstm_units'],\n",
    "            return_sequences=True, \n",
    "            return_state=False,  # Don't return states for simplicity\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.1,\n",
    "            name='decoder_lstm'\n",
    "        )\n",
    "        decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "        \n",
    "        # ===== SIMPLIFIED OUTPUT =====\n",
    "        # Global pooling instead of attention\n",
    "        pooled_output = GlobalMaxPooling1D(name='global_pooling')(decoder_outputs)\n",
    "        \n",
    "        # Dense layers\n",
    "        dense1 = Dense(self.config['dense_units'], activation='relu', name='dense_1')(pooled_output)\n",
    "        outputs = Dense(answer_vocab_size, activation='softmax', name='outputs')(dense1)\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(\n",
    "            inputs=[encoder_inputs, decoder_inputs],\n",
    "            outputs=outputs,\n",
    "            name='simple_health_qa_model'\n",
    "        )\n",
    "        \n",
    "        # Custom optimizer with gradient clipping\n",
    "        optimizer = Adam(\n",
    "            learning_rate=self.config['learning_rate'],\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Simplified model built successfully\")\n",
    "        return model\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, validation_data: Tuple, epochs: int = 15):\n",
    "        \"\"\"Train the simplified model\"\"\"\n",
    "        logger.info(\"Starting simplified model training...\")\n",
    "        \n",
    "        # For this simplified model, we'll use a different approach\n",
    "        # Let's treat it as a classification problem instead of seq2seq\n",
    "        \n",
    "        # Build model if not already built\n",
    "        if self.model is None:\n",
    "            question_vocab_size = len(self.data_processor.question_tokenizer.word_index)\n",
    "            answer_vocab_size = len(self.data_processor.answer_tokenizer.word_index)\n",
    "            self.model = self.build_model(question_vocab_size, answer_vocab_size)\n",
    "        \n",
    "        # Model summary\n",
    "        print(\"\\nSimplified Model Architecture:\")\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Simple callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=2,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # For this simplified approach, let's create dummy decoder inputs\n",
    "        # We'll use zeros as decoder inputs since we're not doing proper seq2seq\n",
    "        decoder_input_data = np.zeros_like(X)\n",
    "        \n",
    "        # Target is the answer sequences\n",
    "        target_data = y\n",
    "        \n",
    "        # Prepare validation data similarly\n",
    "        val_X, val_y = validation_data\n",
    "        val_decoder_input = np.zeros_like(val_X)\n",
    "        \n",
    "        # Training\n",
    "        history = self.model.fit(\n",
    "            [X, decoder_input_data],\n",
    "            target_data,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            epochs=epochs,\n",
    "            validation_data=([val_X, val_decoder_input], val_y),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Simplified model training completed\")\n",
    "        return history\n",
    "\n",
    "class RetrievalSystem:\n",
    "    \"\"\"Efficient retrieval system for health Q&A\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.corpus_texts = []\n",
    "        \n",
    "    def build_index(self, answers: List[str], batch_size: int = 16):\n",
    "        \"\"\"Build FAISS index for efficient retrieval\"\"\"\n",
    "        logger.info(\"Building FAISS index...\")\n",
    "        \n",
    "        self.corpus_texts = answers\n",
    "        \n",
    "        # Encode in batches to manage memory\n",
    "        corpus_embeddings = self.embedder.encode(\n",
    "            self.corpus_texts, \n",
    "            batch_size=batch_size, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        faiss.normalize_L2(corpus_embeddings)\n",
    "        \n",
    "        # Use IndexFlatIP for inner product (cosine similarity)\n",
    "        d = corpus_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(d)\n",
    "        self.index.add(corpus_embeddings)\n",
    "        \n",
    "        logger.info(f\"FAISS index built with {self.index.ntotal} entries\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5, threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"Retrieve similar answers with similarity threshold\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build_index first.\")\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        D, I = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if score >= threshold:\n",
    "                results.append({\n",
    "                    'answer': self.corpus_texts[idx],\n",
    "                    'score': float(score)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "class HealthChatbot:\n",
    "    \"\"\"Main Health Chatbot Class - Focused on Retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        self.config = config or CONFIG\n",
    "        self.data_processor = HealthDataProcessor(\n",
    "            max_sequence_length=self.config['max_sequence_length'],\n",
    "            max_vocab_size=self.config['max_vocab_size']\n",
    "        )\n",
    "        self.generative_model = SimpleHealthModel(self.config)  # Use simplified model\n",
    "        self.retrieval_system = RetrievalSystem()\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def load_and_preprocess_data(self, data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and preprocess the health dataset\"\"\"\n",
    "        logger.info(f\"Loading data from {data_path}\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Basic data info\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'topic' in df.columns:\n",
    "            print(\"\\nTopic distribution:\")\n",
    "            print(df['topic'].value_counts().head(10))\n",
    "        \n",
    "        return self.data_processor.preprocess_dataframe(df)\n",
    "    \n",
    "    def prepare_training_splits(self, df: pd.DataFrame, test_size: float = 0.2, val_size: float = 0.1):\n",
    "        \"\"\"Prepare training, validation, and test splits\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Use smaller subset for quick training\n",
    "        if len(df) > 3000:\n",
    "            df = df.sample(n=3000, random_state=42)  # Use smaller subset\n",
    "            print(f\"Using subset of {len(df)} samples for faster training\")\n",
    "        \n",
    "        # First split: training vs temporary\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Second split: validation vs test\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=val_size/(test_size + val_size), random_state=42\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(train_df)}\")\n",
    "        logger.info(f\"Validation samples: {len(val_df)}\")\n",
    "        logger.info(f\"Test samples: {len(test_df)}\")\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "    def train_generative_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame, epochs: int = 15):\n",
    "        \"\"\"Train the simplified generative model\"\"\"\n",
    "        logger.info(\"Training SIMPLIFIED generative model...\")\n",
    "        \n",
    "        # Prepare questions and answers\n",
    "        train_questions = train_df['Question_clean'].tolist()\n",
    "        train_answers = train_df['Answer_clean'].tolist()\n",
    "        val_questions = val_df['Question_clean'].tolist()\n",
    "        val_answers = val_df['Answer_clean'].tolist()\n",
    "        \n",
    "        # Build tokenizers\n",
    "        self.data_processor.build_tokenizers(train_questions, train_answers)\n",
    "        self.generative_model.data_processor = self.data_processor\n",
    "        \n",
    "        # Convert to sequences\n",
    "        X_train, y_train = self.data_processor.texts_to_sequences(train_questions, train_answers)\n",
    "        X_val, y_val = self.data_processor.texts_to_sequences(val_questions, val_answers)\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}, {y_val.shape}\")\n",
    "        \n",
    "        # Train simplified model\n",
    "        history = self.generative_model.train(X_train, y_train, (X_val, y_val), epochs=epochs)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return history\n",
    "    \n",
    "    def build_retrieval_system(self, df: pd.DataFrame):\n",
    "        \"\"\"Build the retrieval system\"\"\"\n",
    "        logger.info(\"Building retrieval system...\")\n",
    "        answers = df['Answer_clean'].drop_duplicates().tolist()\n",
    "        print(f\"Building index with {len(answers)} unique answers\")\n",
    "        self.retrieval_system.build_index(answers, batch_size=16)\n",
    "    \n",
    "    def generate_answer(self, question: str, use_retrieval: bool = True) -> Dict:\n",
    "        \"\"\"Generate answer for a question - PRIMARY: RETRIEVAL\"\"\"\n",
    "        \n",
    "        # Always use retrieval for now - it's more reliable\n",
    "        retrieved = self.retrieval_system.retrieve(\n",
    "            question, \n",
    "            k=self.config['max_answers'],\n",
    "            threshold=self.config['similarity_threshold']\n",
    "        )\n",
    "        \n",
    "        if not retrieved:\n",
    "            return {\n",
    "                \"answer\": \"I'm not sure about that specific medical question. Please consult a healthcare professional for accurate medical advice.\",\n",
    "                \"score\": 0.0,\n",
    "                \"method\": \"fallback\",\n",
    "                \"source\": None\n",
    "            }\n",
    "        \n",
    "        # Use the best retrieved answer\n",
    "        best_result = retrieved[0]\n",
    "        \n",
    "        return {\n",
    "            \"answer\": best_result['answer'],\n",
    "            \"score\": best_result['score'],\n",
    "            \"method\": \"retrieval\",\n",
    "            \"source\": \"medical knowledge base\"\n",
    "        }\n",
    "    \n",
    "    def evaluate_retrieval(self, test_df: pd.DataFrame, k: int = 5) -> Dict:\n",
    "        \"\"\"Evaluate retrieval system performance\"\"\"\n",
    "        logger.info(\"Evaluating retrieval system...\")\n",
    "        \n",
    "        hits = 0\n",
    "        total = min(50, len(test_df))  # Smaller sample for quick evaluation\n",
    "        \n",
    "        for i, (_, row) in enumerate(test_df.head(total).iterrows()):\n",
    "            question = row['Question_clean']\n",
    "            true_answer = row['Answer_clean']\n",
    "            \n",
    "            retrieved = self.retrieval_system.retrieve(question, k=k)\n",
    "            retrieved_answers = [r['answer'] for r in retrieved]\n",
    "            \n",
    "            # Check if true answer is in retrieved set (exact match)\n",
    "            if true_answer in retrieved_answers:\n",
    "                hits += 1\n",
    "        \n",
    "        recall_at_k = hits / total\n",
    "        return {\"recall@k\": recall_at_k, \"evaluated_samples\": total}\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not history:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "            \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        ax1.plot(history.history['loss'], label='Training Loss')\n",
    "        ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_models(self, model_dir: str = \"health_chatbot_models\"):\n",
    "        \"\"\"Save all models and components\"\"\"\n",
    "        model_dir = Path(model_dir)\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save generative model (if trained)\n",
    "        if self.generative_model.model and self.is_trained:\n",
    "            self.generative_model.model.save(model_dir / \"generative_model.h5\")\n",
    "            print(\"✓ Generative model saved\")\n",
    "        \n",
    "        # Save tokenizers\n",
    "        if self.data_processor.question_tokenizer:\n",
    "            with open(model_dir / \"question_tokenizer.json\", 'w') as f:\n",
    "                f.write(self.data_processor.question_tokenizer.to_json())\n",
    "            with open(model_dir / \"answer_tokenizer.json\", 'w') as f:\n",
    "                f.write(self.data_processor.answer_tokenizer.to_json())\n",
    "            print(\"✓ Tokenizers saved\")\n",
    "        \n",
    "        # Save retrieval system\n",
    "        if self.retrieval_system.index:\n",
    "            faiss.write_index(self.retrieval_system.index, str(model_dir / \"faiss_index.bin\"))\n",
    "            np.save(model_dir / \"corpus_texts.npy\", np.array(self.retrieval_system.corpus_texts))\n",
    "            print(\"✓ Retrieval system saved\")\n",
    "        \n",
    "        # Save config\n",
    "        with open(model_dir / \"config.json\", 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ All models saved to {model_dir}\")\n",
    "    \n",
    "    def load_models(self, model_dir: str = \"health_chatbot_models\"):\n",
    "        \"\"\"Load saved models and components\"\"\"\n",
    "        model_dir = Path(model_dir)\n",
    "        \n",
    "        try:\n",
    "            # Load config\n",
    "            with open(model_dir / \"config.json\", 'r') as f:\n",
    "                loaded_config = json.load(f)\n",
    "                self.config.update(loaded_config)\n",
    "            \n",
    "            # Load tokenizers\n",
    "            if (model_dir / \"question_tokenizer.json\").exists():\n",
    "                with open(model_dir / \"question_tokenizer.json\", 'r') as f:\n",
    "                    self.data_processor.question_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "            if (model_dir / \"answer_tokenizer.json\").exists():\n",
    "                with open(model_dir / \"answer_tokenizer.json\", 'r') as f:\n",
    "                    self.data_processor.answer_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "            \n",
    "            # Load generative model\n",
    "            if (model_dir / \"generative_model.h5\").exists():\n",
    "                self.generative_model.model = tf.keras.models.load_model(\n",
    "                    model_dir / \"generative_model.h5\"\n",
    "                )\n",
    "                self.is_trained = True\n",
    "            \n",
    "            # Load retrieval system\n",
    "            if (model_dir / \"faiss_index.bin\").exists():\n",
    "                self.retrieval_system.index = faiss.read_index(str(model_dir / \"faiss_index.bin\"))\n",
    "                self.retrieval_system.corpus_texts = np.load(\n",
    "                    model_dir / \"corpus_texts.npy\", \n",
    "                    allow_pickle=True\n",
    "                ).tolist()\n",
    "            \n",
    "            print(\"✓ Models loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading models: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION - RETRIEVAL-FOCUSED APPROACH\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function - Focused on reliable retrieval system\"\"\"\n",
    "    \n",
    "    print(\"🚀 Health Q&A Chatbot - Retrieval-Focused Version\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    chatbot = HealthChatbot()\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        print(\"\\n📊 Loading and preprocessing data...\")\n",
    "        df = chatbot.load_and_preprocess_data(\"../dataset/merged_health_dataset.csv\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample processed data:\")\n",
    "        print(df[['Question_clean', 'Answer_clean']].head(2))\n",
    "        \n",
    "        # Build retrieval system (PRIMARY COMPONENT)\n",
    "        print(\"\\n🔍 Building retrieval system...\")\n",
    "        chatbot.build_retrieval_system(df)\n",
    "        \n",
    "        # Test retrieval system immediately\n",
    "        print(\"\\n🧪 Testing retrieval system...\")\n",
    "        test_questions = [\n",
    "            \"What are symptoms of depression?\",\n",
    "            \"How to improve sleep quality?\",\n",
    "            \"What foods are good for heart health?\"\n",
    "        ]\n",
    "        \n",
    "        for question in test_questions:\n",
    "            result = chatbot.retrieval_system.retrieve(question)\n",
    "            print(f\"\\nQuery: '{question}'\")\n",
    "            print(f\"Found {len(result)} results:\")\n",
    "            for i, res in enumerate(result[:2]):\n",
    "                print(f\"  {i+1}. Score: {res['score']:.3f}\")\n",
    "                print(f\"     {res['answer'][:80]}...\")\n",
    "        \n",
    "        # Try simplified generative training (optional)\n",
    "        try:\n",
    "            print(\"\\n🧠 Attempting simplified generative model training...\")\n",
    "            train_df, val_df, test_df = chatbot.prepare_training_splits(df)\n",
    "            history = chatbot.train_generative_model(train_df, val_df, epochs=chatbot.config['epochs'])\n",
    "            chatbot.plot_training_history(history)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Generative training skipped: {e}\")\n",
    "            print(\"🎯 Continuing with retrieval-only system (more reliable)\")\n",
    "        \n",
    "        # Evaluate retrieval system\n",
    "        print(\"\\n📊 Evaluating retrieval system...\")\n",
    "        if 'test_df' in locals():\n",
    "            retrieval_metrics = chatbot.evaluate_retrieval(test_df)\n",
    "            print(f\"Retrieval Recall@{retrieval_metrics['evaluated_samples']}: {retrieval_metrics['recall@k']:.3f}\")\n",
    "        \n",
    "        # Save models\n",
    "        print(\"\\n💾 Saving models...\")\n",
    "        chatbot.save_models()\n",
    "        \n",
    "        # Test the complete chatbot\n",
    "        print(\"\\n🤖 Testing Health Chatbot:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        test_questions = [\n",
    "            \"What are common symptoms of anxiety?\",\n",
    "            \"How can I improve my sleep quality?\",\n",
    "            \"What foods are good for heart health?\",\n",
    "            \"How to manage stress effectively?\",\n",
    "            \"What are the benefits of regular exercise?\"\n",
    "        ]\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            result = chatbot.generate_answer(question)\n",
    "            print(f\"\\n{i}. Q: {question}\")\n",
    "            print(f\"   A: {result['answer'][:120]}...\")\n",
    "            print(f\"   Method: {result['method']}, Score: {result.get('score', 0):.3f}\")\n",
    "        \n",
    "        # Interactive testing\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"💬 Interactive Chat Mode - RETRIEVAL SYSTEM\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\n🤔 Enter your health question: \").strip()\n",
    "            \n",
    "            if question.lower() == 'quit':\n",
    "                break\n",
    "            elif not question:\n",
    "                continue\n",
    "                \n",
    "            result = chatbot.generate_answer(question)\n",
    "            \n",
    "            print(f\"\\n💡 Answer (from medical knowledge base):\")\n",
    "            print(f\"   {result['answer']}\")\n",
    "            if result.get('score'):\n",
    "                print(f\"   Confidence: {result['score']:.3f}\")\n",
    "        \n",
    "        print(\"\\n✅ Health Chatbot ready for use!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {e}\")\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# =============================================================================\n",
    "# QUICK RETRIEVAL-ONLY CHATBOT (RECOMMENDED - FAST & RELIABLE)\n",
    "# =============================================================================\n",
    "\n",
    "def quick_retrieval_chatbot():\n",
    "    \"\"\"Fast retrieval-only chatbot - no training required\"\"\"\n",
    "    print(\"⚡ Quick Retrieval-Only Health Chatbot\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load data and build retrieval system only\n",
    "    chatbot = HealthChatbot()\n",
    "    \n",
    "    try:\n",
    "        df = chatbot.load_and_preprocess_data(\"../dataset/merged_health_dataset.csv\")\n",
    "        chatbot.build_retrieval_system(df)\n",
    "        \n",
    "        print(\"✅ Retrieval system ready!\")\n",
    "        print(\"You can now ask health questions...\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"🤔 Your question (or 'quit'): \").strip()\n",
    "            if question.lower() == 'quit':\n",
    "                break\n",
    "            elif question.lower() == 'demo':\n",
    "                # Demo questions\n",
    "                demo_questions = [\n",
    "                    \"What are symptoms of flu?\",\n",
    "                    \"How to lower blood pressure?\",\n",
    "                    \"Benefits of exercise?\",\n",
    "                    \"What is healthy diet?\"\n",
    "                ]\n",
    "                for q in demo_questions:\n",
    "                    result = chatbot.generate_answer(q)\n",
    "                    print(f\"\\nQ: {q}\")\n",
    "                    print(f\"A: {result['answer'][:100]}...\")\n",
    "                    print(f\"   (Score: {result.get('score', 0):.3f})\")\n",
    "                continue\n",
    "            elif not question:\n",
    "                continue\n",
    "                \n",
    "            result = chatbot.generate_answer(question)\n",
    "            print(f\"\\n💡 {result['answer']}\")\n",
    "            if result.get('score'):\n",
    "                print(f\"   (Confidence: {result['score']:.3f})\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Uncomment for quick retrieval-only chatbot (RECOMMENDED)\n",
    "# quick_retrieval_chatbot()\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND USE PRE-BUILT MODEL\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_chat():\n",
    "    \"\"\"Load pre-built model and chat\"\"\"\n",
    "    print(\"🔮 Loading pre-trained chatbot...\")\n",
    "    \n",
    "    chatbot = HealthChatbot()\n",
    "    \n",
    "    try:\n",
    "        chatbot.load_models(\"health_chatbot_models\")\n",
    "        print(\"✓ Chatbot loaded successfully!\")\n",
    "        print(\"Type 'quit' to exit\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"🤔 Your question: \").strip()\n",
    "            if question.lower() == 'quit':\n",
    "                break\n",
    "                \n",
    "            result = chatbot.generate_answer(question)\n",
    "            print(f\"\\n💡 {result['answer']}\")\n",
    "            if result.get('score'):\n",
    "                print(f\"   (Confidence: {result['score']:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading chatbot: {e}\")\n",
    "        print(\"Please run the training cell first or use quick_retrieval_chatbot()\")\n",
    "\n",
    "# Uncomment to load and use pre-built model\n",
    "# load_and_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba876f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
