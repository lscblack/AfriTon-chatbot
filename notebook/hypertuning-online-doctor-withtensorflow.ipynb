{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356c4b4d",
   "metadata": {},
   "source": [
    "# üè• Health Domain Q&A Chatbot ‚Äî Capstone Project\n",
    "\n",
    "**Author:** Loue Sauveur Christian (Chriss)  \n",
    "**Institution:** African Leadership University  \n",
    "**Domain:** Healthcare (Medical Q&A Chatbot)  \n",
    "**Purpose:**  \n",
    "This project develops a healthcare question-answering chatbot that provides concise, accurate, and safe responses to health-related questions using a generative Transformer model (T5). The chatbot is trained on a large, domain-specific health dataset containing real medical questions and verified answers.\n",
    "\n",
    "**Relevance & Justification:**  \n",
    "- Healthcare misinformation is widespread; this chatbot provides reliable information from verified medical sources.  \n",
    "- It supports users by answering general health questions safely (not replacing doctors).  \n",
    "- Domain alignment ensures the model focuses strictly on health content, rejecting unrelated queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e30c7",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e9a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-16 16:30:49.684121: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-16 16:30:49.703035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760625049.725325  224757 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760625049.732507  224757 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760625049.751175  224757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760625049.751201  224757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760625049.751204  224757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760625049.751206  224757 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-16 16:30:49.756883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Code cell: imports, constants, seed\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP & ML libs\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate  # metrics library\n",
    "\n",
    "# Transformers & sentence embeddings (TensorFlow)\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    TFAutoModelForSeq2SeqLM, \n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Faiss for vector index\n",
    "import faiss\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf8912",
   "metadata": {},
   "source": [
    "## NlKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3aead64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lscblack/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lscblack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run if missing packages (uncomment to install)\n",
    "# !pip install -q nltk transformers datasets sentence-transformers faiss-cpu optuna accelerate rouge_score sacrebleu tensorflow\n",
    "\n",
    "# NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aeade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 16371\n",
      "['Question', 'Answer', 'topic', 'split']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>topic</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>Key Points - Non-small cell lung cancer is a d...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is at risk for Non-Small Cell Lung Cancer? ?</td>\n",
       "      <td>Smoking is the major risk factor for non-small...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptoms of Non-Small Cell Lung C...</td>\n",
       "      <td>Signs of non-small cell lung cancer include a ...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to diagnose Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>Tests that examine the lungs are used to detec...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the outlook for Non-Small Cell Lung Ca...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the stages of Non-Small Cell Lung Can...</td>\n",
       "      <td>Key Points - After lung cancer has been diagno...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
       "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
       "2  What are the symptoms of Non-Small Cell Lung C...   \n",
       "3       How to diagnose Non-Small Cell Lung Cancer ?   \n",
       "4  What is the outlook for Non-Small Cell Lung Ca...   \n",
       "5  What are the stages of Non-Small Cell Lung Can...   \n",
       "\n",
       "                                              Answer   topic  split  \n",
       "0  Key Points - Non-small cell lung cancer is a d...  cancer  train  \n",
       "1  Smoking is the major risk factor for non-small...  cancer  train  \n",
       "2  Signs of non-small cell lung cancer include a ...  cancer   test  \n",
       "3  Tests that examine the lungs are used to detec...  cancer  train  \n",
       "4  Certain factors affect prognosis (chance of re...  cancer  train  \n",
       "5  Key Points - After lung cancer has been diagno...  cancer  train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "topic\n",
       "growth_hormone_receptor          5430\n",
       "Genetic_and_Rare_Diseases        5388\n",
       "Diabetes_Digestive_Kidney        1157\n",
       "Neurological_Disorders_Stroke    1088\n",
       "Other                             981\n",
       "SeniorHealth                      769\n",
       "cancer                            729\n",
       "Heart_Lung_Blood                  559\n",
       "Disease_Control_Prevention        270\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    13089\n",
       "test      3282\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load merged dataset\n",
    "DATA_PATH = \"../dataset/merged_health_dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Quick overview\n",
    "print(\"Rows:\", len(df))\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows (works in Jupyter/Colab)\n",
    "display(df.head(6))\n",
    "\n",
    "# Show top 20 topics\n",
    "display(df['topic'].value_counts().head(20))\n",
    "\n",
    "# Show data split counts (train/test/val)\n",
    "display(df['split'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98418539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dropped 13 exact duplicate rows\n",
      "INFO:__main__:After dropping very short Q/A: 16357 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>topic</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>Key Points - Non-small cell lung cancer is a d...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is at risk for Non-Small Cell Lung Cancer? ?</td>\n",
       "      <td>Smoking is the major risk factor for non-small...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptoms of Non-Small Cell Lung C...</td>\n",
       "      <td>Signs of non-small cell lung cancer include a ...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to diagnose Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>Tests that examine the lungs are used to detec...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the outlook for Non-Small Cell Lung Ca...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "      <td>cancer</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
       "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
       "2  What are the symptoms of Non-Small Cell Lung C...   \n",
       "3       How to diagnose Non-Small Cell Lung Cancer ?   \n",
       "4  What is the outlook for Non-Small Cell Lung Ca...   \n",
       "\n",
       "                                              Answer   topic  split  \n",
       "0  Key Points - Non-small cell lung cancer is a d...  cancer  train  \n",
       "1  Smoking is the major risk factor for non-small...  cancer  train  \n",
       "2  Signs of non-small cell lung cancer include a ...  cancer   test  \n",
       "3  Tests that examine the lungs are used to detec...  cancer  train  \n",
       "4  Certain factors affect prognosis (chance of re...  cancer  train  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows missing Question/Answer\n",
    "df = df.dropna(subset=['Question', 'Answer']).reset_index(drop=True)\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Lowercase and strip topic column\n",
    "df['topic'] = df['topic'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Drop exact duplicates based on Q/A\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=['Question', 'Answer'])\n",
    "logger.info(f\"Dropped {before - len(df)} exact duplicate rows\")\n",
    "\n",
    "# Remove rows with extremely short Questions or Answers\n",
    "df = df[(df['Question'].str.len() > 10) & (df['Answer'].str.len() > 20)].reset_index(drop=True)\n",
    "logger.info(\"After dropping very short Q/A: %d rows\", len(df))\n",
    "\n",
    "# Show a preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235cac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Question_clean</th>\n",
       "      <th>Answer_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>What is (are) Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>Non-small cell lung cancer is a disease in whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is at risk for Non-Small Cell Lung Cancer? ?</td>\n",
       "      <td>Who is at risk for Non-Small Cell Lung Cancer? ?</td>\n",
       "      <td>Smoking is the major risk factor for non-small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptoms of Non-Small Cell Lung C...</td>\n",
       "      <td>What are the symptoms of Non-Small Cell Lung C...</td>\n",
       "      <td>Signs of non-small cell lung cancer include a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to diagnose Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>How to diagnose Non-Small Cell Lung Cancer ?</td>\n",
       "      <td>Tests that examine the lungs are used to detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the outlook for Non-Small Cell Lung Ca...</td>\n",
       "      <td>What is the outlook for Non-Small Cell Lung Ca...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the stages of Non-Small Cell Lung Can...</td>\n",
       "      <td>What are the stages of Non-Small Cell Lung Can...</td>\n",
       "      <td>After lung cancer has been diagnosed, tests ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
       "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
       "2  What are the symptoms of Non-Small Cell Lung C...   \n",
       "3       How to diagnose Non-Small Cell Lung Cancer ?   \n",
       "4  What is the outlook for Non-Small Cell Lung Ca...   \n",
       "5  What are the stages of Non-Small Cell Lung Can...   \n",
       "\n",
       "                                      Question_clean  \\\n",
       "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
       "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
       "2  What are the symptoms of Non-Small Cell Lung C...   \n",
       "3       How to diagnose Non-Small Cell Lung Cancer ?   \n",
       "4  What is the outlook for Non-Small Cell Lung Ca...   \n",
       "5  What are the stages of Non-Small Cell Lung Can...   \n",
       "\n",
       "                                        Answer_clean  \n",
       "0  Non-small cell lung cancer is a disease in whi...  \n",
       "1  Smoking is the major risk factor for non-small...  \n",
       "2  Signs of non-small cell lung cancer include a ...  \n",
       "3  Tests that examine the lungs are used to detec...  \n",
       "4  Certain factors affect prognosis (chance of re...  \n",
       "5  After lung cancer has been diagnosed, tests ar...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning utilities\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords set\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(text)                        # unescape HTML entities\n",
    "    t = re.sub(r'\\n+', ' ', t)                     # remove newlines\n",
    "    t = re.sub(r'\\[.*?\\]', ' ', t)                 # remove bracketed text\n",
    "    t = re.sub(r'Key Points[:\\s-]*', '', t, flags=re.I)  # remove \"Key Points\"\n",
    "    t = re.sub(r'\\s+', ' ', t)                     # collapse whitespace\n",
    "    t = t.strip()\n",
    "    return t\n",
    "\n",
    "# Apply cleaning\n",
    "df['Question_clean'] = df['Question'].apply(clean_text)\n",
    "df['Answer_clean'] = df['Answer'].apply(clean_text)\n",
    "\n",
    "# Optional: lowercase for retrieval / embedding tasks\n",
    "df['Question_norm'] = df['Question_clean'].str.lower()\n",
    "df['Answer_norm'] = df['Answer_clean'].str.strip()\n",
    "\n",
    "# Preview cleaned data\n",
    "df[['Question', 'Question_clean', 'Answer_clean']].head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92ab414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English fraction: 0.971510668215443\n"
     ]
    }
   ],
   "source": [
    "# Optional: detect language and keep English entries\n",
    "# pip install langdetect\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Set seed for reproducibility\n",
    "DetectorFactory.seed = SEED\n",
    "\n",
    "# Function to check if text is English\n",
    "def is_english(s):\n",
    "    try:\n",
    "        return detect(s) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply language detection (small datasets; batch for large ones)\n",
    "df['is_english'] = df['Question_clean'].apply(is_english)\n",
    "\n",
    "# Report fraction of English entries\n",
    "print(\"English fraction:\", df['is_english'].mean())\n",
    "\n",
    "# Keep only English rows\n",
    "df = df[df['is_english']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98173956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "growth_hormone_receptor          5303\n",
       "genetic_and_rare_diseases        5153\n",
       "diabetes_digestive_kidney        1126\n",
       "neurological_disorders_stroke    1082\n",
       "other                             972\n",
       "seniorhealth                      756\n",
       "cancer                            712\n",
       "heart_lung_blood                  529\n",
       "disease_control_prevention        258\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "topic_group\n",
       "growth_hormone_receptor          5303\n",
       "genetic_and_rare_diseases        5153\n",
       "diabetes_digestive_kidney        1126\n",
       "neurological_disorders_stroke    1082\n",
       "other                             972\n",
       "seniorhealth                      756\n",
       "cancer                            712\n",
       "heart_lung_blood                  529\n",
       "disease_control_prevention        258\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize topics: strip punctuation, unify synonyms\n",
    "df['topic'] = df['topic'].str.replace(r'[^a-z0-9_ ]', '', regex=True).str.strip()\n",
    "\n",
    "# Show top topics\n",
    "display(df['topic'].value_counts().head(40))\n",
    "\n",
    "# Optionally bin small topics into 'other' for stratified splitting\n",
    "topic_counts = df['topic'].value_counts()\n",
    "rare_threshold = 50  # tune depending on dataset size\n",
    "rare_topics = topic_counts[topic_counts < rare_threshold].index.tolist()\n",
    "df['topic_group'] = df['topic'].apply(lambda x: 'other' if x in rare_topics else x)\n",
    "\n",
    "# Preview the grouped topics\n",
    "display(df['topic_group'].value_counts().head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8082a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved cleaned CSV to ../models/processed-v7/health_qa_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"../models/processed-v7\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "clean_path = OUT_DIR / \"health_qa_cleaned.csv\"\n",
    "df.to_csv(clean_path, index=False)\n",
    "logger.info(f\"Saved cleaned CSV to {clean_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548fe6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling:\n",
      "Train: 12712 Valid: 1589 Test: 1590\n",
      "\n",
      "Applying oversampling to training data...\n",
      "Oversampling target: 778 samples per class\n",
      "  seniorhealth: 605 ‚Üí 778 (oversampled)\n",
      "  genetic_and_rare_diseases: 4122 (unchanged)\n",
      "  diabetes_digestive_kidney: 901 (unchanged)\n",
      "  growth_hormone_receptor: 4242 (unchanged)\n",
      "  heart_lung_blood: 423 ‚Üí 778 (oversampled)\n",
      "  neurological_disorders_stroke: 865 (unchanged)\n",
      "  cancer: 570 ‚Üí 778 (oversampled)\n",
      "  other: 778 (unchanged)\n",
      "  disease_control_prevention: 206 ‚Üí 778 (oversampled)\n",
      "\n",
      "After oversampling:\n",
      "Train: 14020 Valid: 1589 Test: 1590\n",
      "Balanced datasets saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# -------------------------\n",
    "# Stratified train/validation/test split\n",
    "# -------------------------\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['topic_group'], random_state=SEED\n",
    ")\n",
    "valid_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['topic_group'], random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Before oversampling:\")\n",
    "print(\"Train:\", len(train_df), \"Valid:\", len(valid_df), \"Test:\", len(test_df))\n",
    "\n",
    "# -------------------------\n",
    "# Oversampling to balance classes\n",
    "# -------------------------\n",
    "print(\"\\nApplying oversampling to training data...\")\n",
    "topic_counts = train_df['topic_group'].value_counts()\n",
    "target_size = int(topic_counts.median())  # balance to median class size\n",
    "print(f\"Oversampling target: {target_size} samples per class\")\n",
    "\n",
    "oversampled_dfs = []\n",
    "for topic in train_df['topic_group'].unique():\n",
    "    topic_df = train_df[train_df['topic_group'] == topic]\n",
    "    current_size = len(topic_df)\n",
    "    \n",
    "    if current_size < target_size:\n",
    "        # Oversample minority class\n",
    "        oversampled_df = resample(\n",
    "            topic_df,\n",
    "            replace=True,\n",
    "            n_samples=target_size,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        oversampled_dfs.append(oversampled_df)\n",
    "        print(f\"  {topic}: {current_size} ‚Üí {target_size} (oversampled)\")\n",
    "    else:\n",
    "        # Keep majority class as is\n",
    "        oversampled_dfs.append(topic_df)\n",
    "        print(f\"  {topic}: {current_size} (unchanged)\")\n",
    "\n",
    "# Combine oversampled data and shuffle\n",
    "balanced_train_df = pd.concat(oversampled_dfs, ignore_index=True)\n",
    "balanced_train_df = balanced_train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter oversampling:\")\n",
    "print(\"Train:\", len(balanced_train_df), \"Valid:\", len(valid_df), \"Test:\", len(test_df))\n",
    "\n",
    "# -------------------------\n",
    "# Save CSV splits for TF pipeline\n",
    "# -------------------------\n",
    "balanced_train_df.to_csv(OUT_DIR / \"train.csv\", index=False)\n",
    "valid_df.to_csv(OUT_DIR / \"valid.csv\", index=False)\n",
    "test_df.to_csv(OUT_DIR / \"test.csv\", index=False)\n",
    "\n",
    "print(\"Balanced datasets saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa20c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 11921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5961/5961 [00:34<00:00, 173.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 11921\n",
      "Saved FAISS index and corpus.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Sentence embeddings: for retrieval / similarity search\n",
    "# -------------------------\n",
    "model_name = \"all-MiniLM-L6-v2\"  # small, fast, good for production\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "# Corpus: one passage per Answer (or chunk if very long)\n",
    "corpus_df = balanced_train_df[['Answer_clean']].drop_duplicates().reset_index(drop=True)\n",
    "corpus_texts = corpus_df['Answer_clean'].tolist()\n",
    "print(\"Corpus size:\", len(corpus_texts))\n",
    "\n",
    "# Encode in batches\n",
    "corpus_embeddings = embedder.encode(\n",
    "    corpus_texts, \n",
    "    batch_size=2, \n",
    "    show_progress_bar=True, \n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Build FAISS index (cosine similarity)\n",
    "# -------------------------\n",
    "d = corpus_embeddings.shape[1]\n",
    "\n",
    "# Inner product works as cosine similarity if embeddings are normalized\n",
    "index = faiss.IndexFlatIP(d)\n",
    "\n",
    "# Normalize embeddings to unit length\n",
    "faiss.normalize_L2(corpus_embeddings)\n",
    "index.add(corpus_embeddings)\n",
    "print(\"FAISS index size:\", index.ntotal)\n",
    "\n",
    "# -------------------------\n",
    "# Save index and corpus texts for retrieval\n",
    "# -------------------------\n",
    "faiss.write_index(index, str(OUT_DIR / \"faiss_index.ivf\"))\n",
    "np.save(OUT_DIR / \"corpus_texts.npy\", np.array(corpus_texts))\n",
    "print(\"Saved FAISS index and corpus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a6bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 213.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': \"Signs of non-small cell lung cancer include a cough that doesn't go away and shortness of breath. Sometimes lung cancer does not cause any signs or symptoms. It may be found during a chest x-ray done for another condition. Signs and symptoms may be caused by lung cancer or by other conditions. Check with your doctor if you have any of the following: - Chest discomfort or pain. - A cough that doesnt go away or gets worse over time. - Trouble breathing. - Wheezing. - Blood in sputum (mucus coughed up from the lungs). - Hoarseness. - Loss of appetite. - Weight loss for no known reason. - Feeling very tired. - Trouble swallowing. - Swelling in the face and/or veins in the neck.\", 'score': 0.7313530445098877}, {'answer': \"Lung cancer is one of the most common cancers in the world. It is a leading cause of cancer death in men and women in the United States. Cigarette smoking causes most lung cancers. The more cigarettes you smoke per day and the earlier you started smoking, the greater your risk of lung cancer. High levels of pollution, radiation and asbestos exposure may also increase risk. Common symptoms of lung cancer include - A cough that doesn't go away and gets worse over time - Constant chest pain - Coughing up blood - Shortness of breath, wheezing, or hoarseness - Repeated problems with pneumonia or bronchitis - Swelling of the neck and face - Loss of appetite or weight loss - Fatigue Doctors diagnose lung cancer using a physical exam, imaging, and lab tests. Treatment depends on the type, stage, and how advanced it is. Treatments include surgery, chemotherapy, radiation therapy, and targeted therapy. Targeted therapy uses substances that attack cancer cells without harming normal cells. NIH: National Cancer Institute\", 'score': 0.6900779008865356}, {'answer': \"Lung adenocarcinoma is a cancer that occurs due to abnormal and uncontrolled cell growth in the lungs. It is a subtype of non-small cell lung cancer that is often diagnosed in an outer area of the lung. Early lung cancers may not be associated with any signs and symptoms. As the condition progresses, affected people can experience chest pain, a persistent cough, fatigue, coughing up blood, loss of appetite, unexplained weight loss, shortness of breath, and/or wheezing. The underlying cause of lung adenocarcinoma is generally unknown; however, risk factors for developing a lung cancer include smoking; exposure to secondhand smoke and other toxic chemicals; a family history of lung cancer; previous radiation treatment to the chest or breast; and HIV infection. Treatment varies based on the severity of the condition, the associated signs and symptoms and the affected person's overall health. It may include a combination of surgery, radiation therapy, chemotherapy, targeted therapy, and/or watchful waiting.\", 'score': 0.6838819980621338}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Retrieval function using embeddings + FAISS\n",
    "# -------------------------\n",
    "def retrieve_answers(query, k=5):\n",
    "    # Encode query\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Normalize to unit length for cosine similarity\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    \n",
    "    # Search FAISS index\n",
    "    D, I = index.search(q_emb, k)\n",
    "    \n",
    "    # Collect top-k answers with scores\n",
    "    results = []\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        results.append({\n",
    "            'answer': corpus_texts[idx],\n",
    "            'score': float(score)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "query_example = \"What are early symptoms of lung cancer?\"\n",
    "top_answers = retrieve_answers(query_example, k=3)\n",
    "print(top_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84664d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14020 [00:00<?, ? examples/s]/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14020/14020 [00:02<00:00, 4794.71 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1589/1589 [00:00<00:00, 5149.50 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1590/1590 [00:00<00:00, 4876.74 examples/s]\n",
      "/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/datasets/arrow_dataset.py:405: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF datasets ready!\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# -------------------------\n",
    "# Build HF datasets\n",
    "# -------------------------\n",
    "hf_train = Dataset.from_pandas(balanced_train_df[['Question_clean','Answer_clean','topic_group']])\n",
    "hf_valid = Dataset.from_pandas(valid_df[['Question_clean','Answer_clean','topic_group']])\n",
    "hf_test = Dataset.from_pandas(test_df[['Question_clean','Answer_clean','topic_group']])\n",
    "\n",
    "ds = DatasetDict({\"train\": hf_train, \"validation\": hf_valid, \"test\": hf_test})\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer & model\n",
    "# -------------------------\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "# -------------------------\n",
    "# Preprocessing function\n",
    "# -------------------------\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {q} topic: {t}\" for q, t in zip(examples['Question_clean'], examples['topic_group'])]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            [a if a is not None else \"\" for a in examples[\"Answer_clean\"]],\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "    # Set labels tensor directly\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  # <-- must be a tf.Tensor\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Map preprocessing to datasets\n",
    "# -------------------------\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = hf_train.map(preprocess_function, batched=True, remove_columns=hf_train.column_names)\n",
    "tokenized_valid = hf_valid.map(preprocess_function, batched=True, remove_columns=hf_valid.column_names)\n",
    "tokenized_test = hf_test.map(preprocess_function, batched=True, remove_columns=hf_test.column_names)\n",
    "\n",
    "# -------------------------\n",
    "# Convert to TensorFlow datasets\n",
    "# -------------------------\n",
    "def to_tf_dataset(tokenized_dataset, batch_size=2, shuffle=True):\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    \n",
    "    # Important: use `label_cols=[\"labels\"]` to separate features and labels\n",
    "    tf_ds = tokenized_dataset.to_tf_dataset(\n",
    "        columns=columns,            # features\n",
    "        label_cols=[\"labels\"],       # labels\n",
    "        shuffle=shuffle,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=None,\n",
    "    )\n",
    "    return tf_ds\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "tf_train_ds = to_tf_dataset(tokenized_train, batch_size=batch_size)\n",
    "tf_valid_ds = to_tf_dataset(tokenized_valid, batch_size=batch_size, shuffle=False)\n",
    "tf_test_ds = to_tf_dataset(tokenized_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"TF datasets ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e65411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already\n",
    "# !pip install optuna plotly\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b302367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lscblack/miniconda3/envs/ml-gpu/lib/python3.10/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow training setup ready for google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "# -------------------------\n",
    "# Model\n",
    "# -------------------------\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    from_pt=True,         # convert PyTorch weights to TF\n",
    "    use_safetensors=True  # optional but faster & safer\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer & learning rate\n",
    "# -------------------------\n",
    "learning_rate = 1e-5\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate, epsilon=1e-08\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Loss function\n",
    "# -------------------------\n",
    "# For T5, labels with -100 are masked automatically\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Metrics\n",
    "# -------------------------\n",
    "# Can integrate ROUGE with a custom callback if needed\n",
    "metrics = []  # leave empty; compute ROUGE offline or via custom callback\n",
    "\n",
    "# -------------------------\n",
    "# Compile model\n",
    "# -------------------------\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",      # or a custom metric\n",
    "    patience=6,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(OUT_DIR / \"t5_health_ckpt\"),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "print(f\"TensorFlow training setup ready for {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7dc947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow-ready compute_metrics function is ready!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Metrics\n",
    "# -------------------------\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# -------------------------\n",
    "# Compute metrics function\n",
    "# -------------------------\n",
    "def compute_metrics_for_generation(predictions, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute BLEU, ROUGE, Exact Match, and token-level F1 for generative QA.\n",
    "    \n",
    "    Args:\n",
    "        predictions: np.array or list of token IDs (model outputs)\n",
    "        labels: np.array or list of token IDs (targets)\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    preds = np.array(predictions)\n",
    "    labels_ids = np.array(labels)\n",
    "    \n",
    "    # Replace -100 with pad token\n",
    "    labels_ids = np.where(labels_ids == -100, tokenizer.pad_token_id, labels_ids)\n",
    "    \n",
    "    # Filter invalid tokens (negative or beyond vocab)\n",
    "    def filter_valid_tokens(token_ids, max_id=tokenizer.vocab_size):\n",
    "        valid_tokens = []\n",
    "        for seq in token_ids:\n",
    "            valid_seq = [int(t) if 0 <= int(t) < max_id else tokenizer.pad_token_id for t in seq]\n",
    "            valid_tokens.append(valid_seq)\n",
    "        return np.array(valid_tokens)\n",
    "    \n",
    "    preds = filter_valid_tokens(preds)\n",
    "    labels_ids = filter_valid_tokens(labels_ids)\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # BLEU\n",
    "    try:\n",
    "        bleu_score = bleu.compute(predictions=decoded_preds, references=[[r] for r in decoded_labels])['score']\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    \n",
    "    # ROUGE\n",
    "    try:\n",
    "        rouge_score = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        rouge1 = rouge_score['rouge1']\n",
    "        rouge2 = rouge_score['rouge2']\n",
    "        rougel = rouge_score['rougeL']\n",
    "    except:\n",
    "        rouge1, rouge2, rougel = 0.0, 0.0, 0.0\n",
    "\n",
    "    # Exact Match\n",
    "    try:\n",
    "        em = np.mean([int(p.strip() == l.strip()) for p, l in zip(decoded_preds, decoded_labels)])\n",
    "    except:\n",
    "        em = 0.0\n",
    "\n",
    "    # Token-level F1\n",
    "    def token_f1(a, b):\n",
    "        a_tokens = a.split()\n",
    "        b_tokens = b.split()\n",
    "        common = Counter(a_tokens) & Counter(b_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return 0.0\n",
    "        prec = num_same / max(1, len(a_tokens))\n",
    "        rec = num_same / max(1, len(b_tokens))\n",
    "        return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "    try:\n",
    "        token_f1_mean = float(np.mean([token_f1(p, l) for p, l in zip(decoded_preds, decoded_labels)]))\n",
    "    except:\n",
    "        token_f1_mean = 0.0\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rouge1\": rouge1,\n",
    "        \"rouge2\": rouge2,\n",
    "        \"rougel\": rougel,\n",
    "        \"exact_match\": float(em),\n",
    "        \"token_f1\": token_f1_mean\n",
    "    }\n",
    "\n",
    "print(\"TensorFlow-ready compute_metrics function is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e68403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760625246.102681  225676 service.cc:152] XLA service 0x7a999c345530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1760625246.102706  225676 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-10-16 16:34:06.109412: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1760625246.129955  225676 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1760625246.303328  225676 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14020/14020 [==============================] - ETA: 0s - loss: 2.4555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Sharding callback duration: 181 microseconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "# -------------------------\n",
    "# Load model\n",
    "# -------------------------\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer & compile\n",
    "# -------------------------\n",
    "learning_rate = 1e-5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(OUT_DIR / \"t5_health_final_ckpt\"),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Metrics callback (TF-style)\n",
    "# -------------------------\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_dataset, tokenizer, max_length=128, num_beams=2):\n",
    "        self.val_dataset = val_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_beams = num_beams\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred_texts = []\n",
    "        label_texts = []\n",
    "\n",
    "        for batch_inputs, batch_labels in self.val_dataset:\n",
    "            batch_size = batch_inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                input_ids = tf.expand_dims(batch_inputs[\"input_ids\"][i], 0)\n",
    "                attention_mask = tf.expand_dims(batch_inputs[\"attention_mask\"][i], 0)\n",
    "\n",
    "                y_pred = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=self.max_length,\n",
    "                    num_beams=self.num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "                pred_texts.append(self.tokenizer.decode(y_pred[0], skip_special_tokens=True))\n",
    "                label_texts.append(self.tokenizer.decode(batch_labels[i], skip_special_tokens=True))\n",
    "\n",
    "        metrics = compute_metrics_for_generation(pred_texts, label_texts, self.tokenizer)\n",
    "        print(f\"\\nEpoch {epoch+1} metrics: {metrics}\")\n",
    "\n",
    "metrics_cb = MetricsCallback(tf_valid_ds, tokenizer)\n",
    "\n",
    "# -------------------------\n",
    "# Train model\n",
    "# -------------------------\n",
    "epochs = 4\n",
    "model.fit(\n",
    "    tf_train_ds,\n",
    "    validation_data=tf_valid_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping_cb, checkpoint_cb, metrics_cb],\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Save model & tokenizer\n",
    "# -------------------------\n",
    "model.save_pretrained(OUT_DIR / \"t5_health_final\")\n",
    "tokenizer.save_pretrained(OUT_DIR / \"t5_health_final_tokenizer\")\n",
    "\n",
    "print(\"TensorFlow generative chatbot training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3077733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Evaluate model on test set\n",
    "# -------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "pred_texts = []\n",
    "label_texts = []\n",
    "\n",
    "# Loop over TF dataset\n",
    "for batch_inputs, batch_labels in tqdm(tf_test_ds, desc=\"Evaluating\"):\n",
    "    batch_size = batch_inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Prepare single example for generation\n",
    "        input_ids = tf.expand_dims(batch_inputs[\"input_ids\"][i], 0)\n",
    "        attention_mask = tf.expand_dims(batch_inputs[\"attention_mask\"][i], 0)\n",
    "\n",
    "        # Generate prediction\n",
    "        y_pred = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,  # shorter for faster generation\n",
    "            num_beams=2,     # lower beam count\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode predicted tokens to text\n",
    "        pred_text = tokenizer.decode(y_pred[0], skip_special_tokens=True)\n",
    "        label_text = tokenizer.decode(batch_labels[i], skip_special_tokens=True)\n",
    "\n",
    "        pred_texts.append(pred_text)\n",
    "        label_texts.append(label_text)\n",
    "\n",
    "# -------------------------\n",
    "# Compute metrics\n",
    "# -------------------------\n",
    "metrics = compute_metrics_for_generation(pred_texts, label_texts, tokenizer)\n",
    "\n",
    "print(\"Evaluation metrics on test set:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calc_perplexity_tf(model, tokenizer, dataset, batch_size=2, max_length=256):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a generative seq2seq model in TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        model: TFAutoModelForSeq2SeqLM\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        dataset: list/dict with 'Question_clean' and 'Answer_clean'\n",
    "        batch_size: evaluation batch size\n",
    "        max_length: maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "        float: perplexity\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Calculating Perplexity\"):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "\n",
    "        # Encode inputs\n",
    "        inputs = tokenizer(\n",
    "            [item[\"Question_clean\"] for item in batch],\n",
    "            return_tensors=\"tf\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        # Encode labels\n",
    "        labels = tokenizer(\n",
    "            [item[\"Answer_clean\"] for item in batch],\n",
    "            return_tensors=\"tf\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).input_ids\n",
    "\n",
    "        # Mask padding tokens\n",
    "        label_mask = tf.not_equal(labels, tokenizer.pad_token_id)\n",
    "        masked_labels = tf.where(label_mask, labels, -100)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=masked_labels, training=False)\n",
    "\n",
    "        # Compute batch loss weighted by token count\n",
    "        batch_token_count = tf.reduce_sum(tf.cast(label_mask, tf.float32))\n",
    "        batch_loss = outputs.loss * batch_token_count\n",
    "\n",
    "        total_loss += batch_loss.numpy()\n",
    "        total_tokens += batch_token_count.numpy()\n",
    "\n",
    "    avg_nll = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_nll)\n",
    "    return perplexity\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "# ppl = calc_perplexity_tf(model, tokenizer, hf_test)\n",
    "# print(\"Perplexity (approx):\", ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load cross-encoder reranker\n",
    "cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(cross_model_name)\n",
    "\n",
    "def answer_with_retrieval_and_generate_tf(question, top_k=5, gen_model=None, threshold=0.35, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Retrieve top passages using FAISS + cross-encoder reranking, then generate an answer.\n",
    "    \n",
    "    Args:\n",
    "        question (str): User query\n",
    "        top_k (int): Number of top passages to retrieve\n",
    "        gen_model: TensorFlow seq2seq model (TFAutoModelForSeq2SeqLM)\n",
    "        threshold (float): Minimum reranker score to trust context\n",
    "        tokenizer: HF tokenizer corresponding to gen_model\n",
    "    \n",
    "    Returns:\n",
    "        dict: {answer, score, source (optional)}\n",
    "    \"\"\"\n",
    "    # 1Ô∏è‚É£ Retrieval: encode question and search FAISS\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    candidates = [corpus_texts[idx] for idx in I[0]]\n",
    "\n",
    "    # 2Ô∏è‚É£ Reranking with cross-encoder\n",
    "    pairs = [[question, c] for c in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    best_score = float(scores[best_idx])\n",
    "    best_passage = candidates[best_idx]\n",
    "\n",
    "    # 3Ô∏è‚É£ Check confidence threshold\n",
    "    if best_score < threshold:\n",
    "        return {\n",
    "            \"answer\": \"I'm not sure about that. Please consult a health professional or provide more details.\",\n",
    "            \"score\": best_score\n",
    "        }\n",
    "\n",
    "    # 4Ô∏è‚É£ Generation\n",
    "    if gen_model is not None and tokenizer is not None:\n",
    "        prompt = f\"Answer this health question: {question} [CONTEXT: {best_passage}]\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"tf\", truncation=True, padding=True, max_length=256)\n",
    "\n",
    "        # Generate output\n",
    "        out = gen_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "            num_beams=8,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            length_penalty=1.0,\n",
    "            temperature=0.8,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        gen_ans = tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
    "        return {\"answer\": gen_ans, \"score\": best_score, \"source\": best_passage}\n",
    "\n",
    "    # Fallback: return the best passage\n",
    "    return {\"answer\": best_passage, \"score\": best_score}\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "# Ensure gen_model is your TF model and tokenizer is passed\n",
    "# gen_model = model  # TFAutoModelForSeq2SeqLM\n",
    "# tokenizer = tokenizer\n",
    "print(answer_with_retrieval_and_generate_tf(\"What are symptoms of depression?\", top_k=5, gen_model=model, tokenizer=tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Example evaluation suite\n",
    "# -------------------------\n",
    "in_domain_examples = [\n",
    "    \"What are common symptoms of anxiety?\",\n",
    "    \"How long does antidepressant medication take to work?\",\n",
    "    \"I feel hopeless and can't sleep, what should I do?\",\n",
    "]\n",
    "\n",
    "out_of_domain_examples = [\n",
    "    \"What's the best GPU for gaming?\",\n",
    "    \"How do I cook rice?\",\n",
    "    \"Explain football offside rule\",\n",
    "]\n",
    "\n",
    "all_examples = in_domain_examples + out_of_domain_examples\n",
    "results = []\n",
    "\n",
    "for question in all_examples:\n",
    "    res = answer_with_retrieval_and_generate_tf(\n",
    "        question,\n",
    "        top_k=5,\n",
    "        gen_model=model,        # TFAutoModelForSeq2SeqLM\n",
    "        tokenizer=tokenizer,\n",
    "        threshold=0.35          # fallback if irrelevant\n",
    "    )\n",
    "    results.append((question, res['answer'], res.get('score')))\n",
    "\n",
    "# -------------------------\n",
    "# Display results\n",
    "# -------------------------\n",
    "for q, a, s in results:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"Score: {s:.4f}\" if s is not None else \"Score: N/A\")\n",
    "    print(f\"A: {a[:400]}\")  # show first 400 chars\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1Ô∏è‚É£ Save SentenceTransformer embeddings\n",
    "# -------------------------\n",
    "embedder.save(str(OUT_DIR / \"embedder_allMiniLM\"))\n",
    "\n",
    "# -------------------------\n",
    "# 2Ô∏è‚É£ Save FAISS index and corpus texts\n",
    "# -------------------------\n",
    "faiss.write_index(index, str(OUT_DIR / \"faiss.index\"))\n",
    "np.save(OUT_DIR / \"corpus_texts.npy\", np.array(corpus_texts))\n",
    "\n",
    "# -------------------------\n",
    "# 3Ô∏è‚É£ Save reranker info\n",
    "# -------------------------\n",
    "# CrossEncoders are not fully saveable in the same way as sentence-transformers.\n",
    "# Recommended: store model name and optionally configuration\n",
    "reranker_config = {\"model_name\": cross_model_name}\n",
    "with open(OUT_DIR / \"reranker_config.json\", \"w\") as f:\n",
    "    json.dump(reranker_config, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# 4Ô∏è‚É£ Save seq2seq model + tokenizer\n",
    "# -------------------------\n",
    "model.save_pretrained(OUT_DIR / \"t5_health_final\")\n",
    "tokenizer.save_pretrained(OUT_DIR / \"t5_health_final_tokenizer\")\n",
    "\n",
    "# -------------------------\n",
    "# Completion message\n",
    "# -------------------------\n",
    "print(f\"All artifacts saved under {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ef161",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b44bbda4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5fb9b37",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
